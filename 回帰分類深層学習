import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.datasets import make_classification, make_regression

# 1. データセット作成
# 回帰データ
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# 分類データ
X_clf, y_clf = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)
X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

# 深層学習データ (分類用: MNIST形式模倣)
X_dl = np.random.rand(1000, 28, 28)  # ランダム生成 (28x28画像データ)
y_dl = np.random.randint(0, 10, size=(1000,))  # ラベル (0~9)
X_dl_train, X_dl_test, y_dl_train, y_dl_test = train_test_split(X_dl, y_dl, test_size=0.2, random_state=42)
y_dl_train = to_categorical(y_dl_train, num_classes=10)  # One-hotエンコーディング
y_dl_test = to_categorical(y_dl_test, num_classes=10)

# 2. 回帰タスク
print("=== 回帰タスク ===")
reg_model = LinearRegression()
reg_model.fit(X_reg_train, y_reg_train)
y_reg_pred = reg_model.predict(X_reg_test)
mse = mean_squared_error(y_reg_test, y_reg_pred)
r2 = r2_score(y_reg_test, y_reg_pred)
print(f"Mean Squared Error: {mse:.4f}")
print(f"R2 Score: {r2:.4f}")

# 3. 分類タスク
print("\n=== 分類タスク ===")
clf_model = DecisionTreeClassifier(max_depth=5)
clf_model.fit(X_clf_train, y_clf_train)
y_clf_pred = clf_model.predict(X_clf_test)
accuracy = accuracy_score(y_clf_test, y_clf_pred)
print(f"Accuracy: {accuracy:.4f}")

# 4. 深層学習タスク
print("\n=== 深層学習タスク ===")
dl_model = Sequential([
    Flatten(input_shape=(28, 28)),            # 入力層
    Dense(128, activation='relu'),           # 隠れ層1
    Dropout(0.2),                            # ドロップアウト
    Dense(64, activation='relu'),            # 隠れ層2
    Dense(10, activation='softmax')          # 出力層 (10クラス分類)
])
dl_model.compile(optimizer='adam',
                 loss='categorical_crossentropy',
                 metrics=['accuracy'])

history = dl_model.fit(X_dl_train, y_dl_train, epochs=10, batch_size=32, verbose=0, validation_split=0.2)
loss, dl_accuracy = dl_model.evaluate(X_dl_test, y_dl_test, verbose=0)
print(f"Deep Learning Test Accuracy: {dl_accuracy:.4f}")

# 5. 学習曲線プロット (深層学習)
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
# 損失曲線
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.legend()

# 精度曲線
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy Curve')
plt.legend()

plt.show()
