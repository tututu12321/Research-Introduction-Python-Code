import torch
import torch.nn as nn
import torch.optim as optim

# データセット（ANDゲートとORゲートの入力と出力）
# 入力の各行は[入力1, 入力2]で、出力は期待される結果（0または1）
data = {
    "AND": {
        "inputs": torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32),
        "outputs": torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)
    },
    "OR": {
        "inputs": torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32),
        "outputs": torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)
    }
}

# ニューラルネットワークの定義（2入力1出力のシンプルなモデル）
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer = nn.Sequential(
            nn.Linear(2, 4),
            nn.ReLU(),
            nn.Linear(4, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.layer(x)

# 学習関数
def train_model(model, inputs, targets, epochs=1000):
    criterion = nn.BCELoss()  # 二値交差エントロピー誤差を使用
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        if epoch % 200 == 0:
            print(f"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}")

# ANDゲートとORゲートの学習と予測
for gate, data in data.items():
    print(f"\nTraining for {gate} gate:")
    model = SimpleNN()
    train_model(model, data["inputs"], data["outputs"])
    
    # 学習後にテストデータを使って出力を確認
    with torch.no_grad():
        predictions = model(data["inputs"])
        print(f"{gate} gate predictions:")
        print(predictions.round())
