import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression, make_blobs
import gym
from collections import defaultdict

# 1. 教師あり学習
# 1.1 回帰
print("=== 教師あり学習: 回帰 ===")
X_reg, y_reg = make_regression(n_samples=500, n_features=5, noise=0.1, random_state=42)
X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

reg_model = LinearRegression()
reg_model.fit(X_reg_train, y_reg_train)
y_reg_pred = reg_model.predict(X_reg_test)

mse = mean_squared_error(y_reg_test, y_reg_pred)
r2 = r2_score(y_reg_test, y_reg_pred)
print(f"Mean Squared Error: {mse:.4f}")
print(f"R2 Score: {r2:.4f}")

# 1.2 分類
print("\n=== 教師あり学習: 分類 ===")
X_clf, y_clf = make_classification(n_samples=500, n_features=5, n_classes=2, random_state=42)
X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

clf_model = LogisticRegression()
clf_model.fit(X_clf_train, y_clf_train)
y_clf_pred = clf_model.predict(X_clf_test)

accuracy = accuracy_score(y_clf_test, y_clf_pred)
print(f"Accuracy: {accuracy:.4f}")

# 2. 教師なし学習
# 2.1 クラスタリング
print("\n=== 教師なし学習: クラスタリング ===")
X_cluster, _ = make_blobs(n_samples=500, centers=3, random_state=42)
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X_cluster)

plt.figure(figsize=(6, 6))
plt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=kmeans.labels_, cmap='viridis', s=50)
plt.title("K-Means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# 2.2 次元削減
print("\n=== 教師なし学習: 次元削減 ===")
X_pca = np.random.rand(500, 10)  # ランダム高次元データ
pca = PCA(n_components=2)
X_pca_reduced = pca.fit_transform(X_pca)

plt.figure(figsize=(6, 6))
plt.scatter(X_pca_reduced[:, 0], X_pca_reduced[:, 1], color='blue', alpha=0.5)
plt.title("PCA: Dimensionality Reduction")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# 3. 強化学習
print("\n=== 強化学習 ===")
env = gym.make('FrozenLake-v1', is_slippery=False)  # Frozen Lake 環境
Q = defaultdict(lambda: np.zeros(env.action_space.n))
alpha = 0.1
gamma = 0.99
epsilon = 0.1
num_episodes = 1000

# Q-learningアルゴリズム
for episode in range(num_episodes):
    state = env.reset()
    done = False
    while not done:
        if np.random.rand() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q[state])

        next_state, reward, done, _ = env.step(action)
        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
        state = next_state

# 最適ポリシーの表示
optimal_policy = {state: np.argmax(actions) for state, actions in Q.items()}
print("Optimal Policy:")
print(optimal_policy)

# FrozenLake 環境の評価
state = env.reset()
done = False
print("\nFrozenLake Path:")
while not done:
    action = np.argmax(Q[state])
    print(f"State: {state}, Action: {action}")
    state, _, done, _ = env.step(action)
env.render()
